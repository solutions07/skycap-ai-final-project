#!/usr/bin/env python3
"""
search_index.py

Utility to load a semantic index (generated by build_index.py) and perform
cosine-similarity search to find the most relevant document(s) for a query.

Supports .pkl and .json index formats. Embeddings are expected to be normalized
(as produced by build_index.py), allowing dot-product as cosine similarity.

CLI usage examples:
  python3 search_index.py --index semantic_index.pkl --query "What is the mission?"
  python3 search_index.py --index semantic_index.json --query "Total assets 2023" --k 3
"""

from __future__ import annotations

import argparse
import json
import logging
import os
import pickle
from typing import Any, Dict, List, Tuple

import numpy as np

try:
    from sentence_transformers import SentenceTransformer
except Exception:  # pragma: no cover
    SentenceTransformer = None  # type: ignore


class SemanticSearcher:
    """Load a semantic index and provide top-k retrieval for queries."""

    def __init__(self, index_path: str | None = None):
        self.index_path = index_path or self._default_index_path()
        self.model_name: str = ""
        self.documents: List[Dict[str, Any]] = []
        self.embeddings: np.ndarray | None = None
        self.model: Any = None

        self._load_index()
        self._load_model()

    def available(self) -> bool:
        # Available only when we have documents, embeddings, and a loaded model
        return bool(self.documents) and (self.embeddings is not None) and (self.model is not None)

    def _default_index_path(self) -> str | None:
        # Prefer pickle, then JSON
        for cand in ("semantic_index.pkl", "semantic_index.json"):
            if os.path.exists(cand):
                return cand
        return None

    def _load_index(self) -> None:
        if not self.index_path:
            return
        path = self.index_path
        try:
            if path.endswith(".json"):
                with open(path, "r", encoding="utf-8") as f:
                    payload = json.load(f)
            else:
                with open(path, "rb") as f:
                    payload = pickle.load(f)
        except Exception as e:
            logging.error("Failed to load index %s: %s", path, e)
            return

        self.model_name = payload.get("model") or "sentence-transformers/all-MiniLM-L6-v2"
        self.documents = payload.get("documents", [])
        embs = payload.get("embeddings")
        try:
            self.embeddings = np.asarray(embs, dtype=np.float32) if embs is not None else None
        except Exception:
            self.embeddings = None

    def _load_model(self) -> None:
        if SentenceTransformer is None:
            logging.warning("sentence-transformers unavailable; semantic search disabled.")
            return
        try:
            self.model = SentenceTransformer(self.model_name)
        except Exception as e:
            logging.error("Failed to load SentenceTransformer model %s: %s", self.model_name, e)
            self.model = None

    def _embed(self, texts: List[str]) -> np.ndarray:
        if not self.model:
            raise RuntimeError("Model not loaded")
        vecs = self.model.encode(
            texts,
            batch_size=32,
            show_progress_bar=False,
            convert_to_numpy=True,
            normalize_embeddings=True,
        )
        return np.asarray(vecs, dtype=np.float32)

    def search(self, query: str, k: int = 1) -> List[Tuple[float, Dict[str, Any]]]:
        if not query or not self.available():
            return []
        try:
            q_vec = self._embed([query])  # shape (1, d)
            embs = self.embeddings
            if embs is None or q_vec is None:
                return []
            # both are normalized, use dot product
            scores = embs @ q_vec[0]
            # top-k indices
            k = max(1, min(k, len(self.documents)))
            top_idx = np.argpartition(scores, -k)[-k:]
            # sort descending
            top_sorted = top_idx[np.argsort(scores[top_idx])[::-1]]
            results: List[Tuple[float, Dict[str, Any]]] = [
                (float(scores[i]), self.documents[i]) for i in top_sorted
            ]
            return results
        except Exception as e:
            logging.error("Semantic search failed: %s", e)
            return []


def _parse_args() -> argparse.Namespace:
    ap = argparse.ArgumentParser(description="Query a semantic index for most relevant documents")
    ap.add_argument("--index", required=True, help="Path to semantic_index.pkl or .json")
    ap.add_argument("--query", required=True, help="User query text")
    ap.add_argument("--k", type=int, default=1, help="Top-k results to return")
    return ap.parse_args()


def main() -> int:  # pragma: no cover
    args = _parse_args()
    logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")
    searcher = SemanticSearcher(args.index)
    if not searcher.available():
        logging.error("Index or model not available.")
        return 1
    results = searcher.search(args.query, k=args.k)
    if not results:
        print("No results.")
        return 0
    for score, doc in results:
        print(f"score={score:.4f} | {doc.get('text')}")
    return 0


if __name__ == "__main__":  # pragma: no cover
    raise SystemExit(main())
